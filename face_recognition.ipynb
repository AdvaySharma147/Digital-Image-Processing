{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tried Using Green Boxes**\n",
    "\n",
    "Perfect for recognition but fails in identification with position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize variables for face tracking\n",
    "face_trackers = {}  # Dictionary to track each face and its identifier\n",
    "next_face_id = 0    # ID counter for assigning IDs to new faces\n",
    "\n",
    "# Load the pre-trained Haar Cascade classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Function to assign face ID or retrieve existing ID based on face location\n",
    "def assign_or_return_face_id(x, y, w, h):\n",
    "    global next_face_id, face_trackers\n",
    "    \n",
    "    # Calculate the center of the detected face\n",
    "    face_center_x = x + w // 2\n",
    "    face_center_y = y + h // 2\n",
    "    \n",
    "    # Initialize variables for face matching\n",
    "    matched_fid = None\n",
    "    min_distance = float('inf')\n",
    "    \n",
    "    # Iterate through existing trackers to find matching face or assign new ID\n",
    "    for fid, (prev_x, prev_y, prev_w, prev_h) in face_trackers.items():\n",
    "        # Calculate Euclidean distance between face centers\n",
    "        distance = np.sqrt((face_center_x - (prev_x + prev_w // 2))**2 + (face_center_y - (prev_y + prev_h // 2))**2)\n",
    "        \n",
    "        # If the distance is small enough, consider it the same face\n",
    "        if distance < 50:  # Adjust this threshold based on your camera setup\n",
    "            matched_fid = fid\n",
    "            break\n",
    "    \n",
    "    # If no matching face is found, assign a new ID\n",
    "    if matched_fid is None:\n",
    "        face_trackers[next_face_id] = (x, y, w, h)\n",
    "        matched_fid = next_face_id\n",
    "        next_face_id += 1\n",
    "    \n",
    "    return matched_fid\n",
    "\n",
    "# Open video capture from default camera (usually index 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame from camera. Check camera connection.\")\n",
    "        break\n",
    "\n",
    "    # Convert frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the grayscale frame\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # Iterate over detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Assign or retrieve face ID based on face location\n",
    "        fid = assign_or_return_face_id(x, y, w, h)\n",
    "\n",
    "        # Draw a green rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Display face ID on the frame\n",
    "        cv2.putText(frame, f'Person {fid}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with rectangles and face IDs\n",
    "    cv2.imshow('Face Tracking', frame)\n",
    "\n",
    "    # Exit the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pose detection using the YOLO model**\n",
    "\n",
    " For detecting keypoints and MediaPipe for calculating angles and poses. This will solve the previous problem of unique identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.093291015625\n",
      "\n",
      "0: 480x640 1 person, 327.9ms\n",
      "Speed: 15.7ms preprocess, 327.9ms inference, 9.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "28.1089453125\n",
      "\n",
      "0: 480x640 1 person, 310.6ms\n",
      "Speed: 0.0ms preprocess, 310.6ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "32.376842447916665\n",
      "\n",
      "0: 480x640 1 person, 376.9ms\n",
      "Speed: 7.3ms preprocess, 376.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "30.217418619791665\n",
      "\n",
      "0: 480x640 1 person, 242.5ms\n",
      "Speed: 7.1ms preprocess, 242.5ms inference, 14.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "27.228860677083333\n",
      "\n",
      "0: 480x640 1 person, 219.8ms\n",
      "Speed: 2.5ms preprocess, 219.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "27.219895833333332\n",
      "\n",
      "0: 480x640 1 person, 231.0ms\n",
      "Speed: 0.0ms preprocess, 231.0ms inference, 16.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "16.355755208333335\n",
      "\n",
      "0: 480x640 1 person, 227.7ms\n",
      "Speed: 5.0ms preprocess, 227.7ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "25.689736328125\n",
      "\n",
      "0: 480x640 1 person, 238.2ms\n",
      "Speed: 0.0ms preprocess, 238.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "25.023665364583334\n",
      "\n",
      "0: 480x640 1 person, 260.2ms\n",
      "Speed: 4.2ms preprocess, 260.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "11.320602213541667\n",
      "\n",
      "0: 480x640 1 person, 242.4ms\n",
      "Speed: 7.5ms preprocess, 242.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "17.803723958333332\n",
      "\n",
      "0: 480x640 1 person, 225.4ms\n",
      "Speed: 7.7ms preprocess, 225.4ms inference, 8.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "19.836578776041666\n",
      "\n",
      "0: 480x640 1 person, 298.5ms\n",
      "Speed: 8.2ms preprocess, 298.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "28.487555338541668\n",
      "\n",
      "0: 480x640 1 person, 243.6ms\n",
      "Speed: 0.0ms preprocess, 243.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "21.455\n",
      "\n",
      "0: 480x640 1 person, 233.1ms\n",
      "Speed: 0.0ms preprocess, 233.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "22.177444661458335\n",
      "\n",
      "0: 480x640 1 person, 215.7ms\n",
      "Speed: 9.0ms preprocess, 215.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "17.994716796875\n",
      "\n",
      "0: 480x640 1 person, 222.3ms\n",
      "Speed: 2.0ms preprocess, 222.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "18.225940755208335\n",
      "\n",
      "0: 480x640 1 person, 232.6ms\n",
      "Speed: 0.0ms preprocess, 232.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "18.15384765625\n",
      "\n",
      "0: 480x640 1 person, 234.1ms\n",
      "Speed: 0.0ms preprocess, 234.1ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "16.203951822916668\n",
      "\n",
      "0: 480x640 1 person, 230.6ms\n",
      "Speed: 8.1ms preprocess, 230.6ms inference, 6.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "21.019658203125\n",
      "\n",
      "0: 480x640 1 person, 223.4ms\n",
      "Speed: 7.5ms preprocess, 223.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "17.798258463541668\n",
      "\n",
      "0: 480x640 1 person, 222.6ms\n",
      "Speed: 0.0ms preprocess, 222.6ms inference, 8.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "17.532366536458333\n",
      "\n",
      "0: 480x640 1 person, 232.0ms\n",
      "Speed: 0.0ms preprocess, 232.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "18.147415364583335\n",
      "\n",
      "0: 480x640 1 person, 227.5ms\n",
      "Speed: 1.5ms preprocess, 227.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "17.23892578125\n",
      "\n",
      "0: 480x640 1 person, 227.4ms\n",
      "Speed: 0.0ms preprocess, 227.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "16.645794270833335\n",
      "\n",
      "0: 480x640 1 person, 224.9ms\n",
      "Speed: 2.2ms preprocess, 224.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "18.999088541666666\n",
      "\n",
      "0: 480x640 1 person, 226.1ms\n",
      "Speed: 8.2ms preprocess, 226.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "16.280126953125\n",
      "\n",
      "0: 480x640 1 person, 218.0ms\n",
      "Speed: 8.1ms preprocess, 218.0ms inference, 6.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "20.487802734375\n",
      "\n",
      "0: 480x640 1 person, 241.8ms\n",
      "Speed: 2.6ms preprocess, 241.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "19.899765625\n",
      "\n",
      "0: 480x640 1 person, 241.6ms\n",
      "Speed: 2.2ms preprocess, 241.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "22.143225911458334\n",
      "\n",
      "0: 480x640 1 person, 237.4ms\n",
      "Speed: 6.0ms preprocess, 237.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "29.291513671875\n",
      "\n",
      "0: 480x640 1 person, 238.1ms\n",
      "Speed: 4.5ms preprocess, 238.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "37.91245442708333\n",
      "\n",
      "0: 480x640 1 person, 230.2ms\n",
      "Speed: 0.0ms preprocess, 230.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "32.87287760416667\n",
      "\n",
      "0: 480x640 1 person, 224.8ms\n",
      "Speed: 0.0ms preprocess, 224.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "32.01484700520833\n",
      "\n",
      "0: 480x640 1 person, 300.2ms\n",
      "Speed: 2.6ms preprocess, 300.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "23.631728515625\n",
      "\n",
      "0: 480x640 1 person, 216.0ms\n",
      "Speed: 8.2ms preprocess, 216.0ms inference, 16.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "19.86276692708333\n",
      "\n",
      "0: 480x640 1 person, 215.5ms\n",
      "Speed: 0.0ms preprocess, 215.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "18.077486979166668\n",
      "\n",
      "0: 480x640 1 person, 242.9ms\n",
      "Speed: 0.0ms preprocess, 242.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "15.343727213541667\n",
      "\n",
      "0: 480x640 1 person, 224.0ms\n",
      "Speed: 8.2ms preprocess, 224.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "24.722897135416666\n",
      "\n",
      "0: 480x640 1 person, 227.9ms\n",
      "Speed: 8.0ms preprocess, 227.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "34.62576822916667\n",
      "\n",
      "0: 480x640 1 person, 290.6ms\n",
      "Speed: 8.0ms preprocess, 290.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "27.694654947916668\n",
      "\n",
      "0: 480x640 1 person, 212.7ms\n",
      "Speed: 0.0ms preprocess, 212.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "39.54882161458333\n",
      "\n",
      "0: 480x640 1 person, 243.7ms\n",
      "Speed: 9.0ms preprocess, 243.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "26.858121744791667\n",
      "\n",
      "0: 480x640 1 person, 225.1ms\n",
      "Speed: 6.5ms preprocess, 225.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "25.02569661458333\n",
      "\n",
      "0: 480x640 1 person, 206.2ms\n",
      "Speed: 7.0ms preprocess, 206.2ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "20.838974609375\n",
      "\n",
      "0: 480x640 1 person, 241.9ms\n",
      "Speed: 2.0ms preprocess, 241.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "23.357721354166667\n",
      "\n",
      "0: 480x640 1 person, 207.8ms\n",
      "Speed: 3.7ms preprocess, 207.8ms inference, 5.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "23.355104166666667\n",
      "\n",
      "0: 480x640 1 person, 216.1ms\n",
      "Speed: 0.0ms preprocess, 216.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "28.396484375\n",
      "\n",
      "0: 480x640 1 person, 221.3ms\n",
      "Speed: 0.0ms preprocess, 221.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "28.883893229166667\n",
      "\n",
      "0: 480x640 1 person, 215.3ms\n",
      "Speed: 0.0ms preprocess, 215.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "25.568040364583332\n",
      "\n",
      "0: 480x640 1 person, 225.6ms\n",
      "Speed: 0.0ms preprocess, 225.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "29.282604166666665\n",
      "\n",
      "0: 480x640 1 person, 223.4ms\n",
      "Speed: 0.0ms preprocess, 223.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "25.37708984375\n",
      "\n",
      "0: 480x640 1 person, 229.8ms\n",
      "Speed: 0.0ms preprocess, 229.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "24.386822916666667\n",
      "\n",
      "0: 480x640 1 person, 220.1ms\n",
      "Speed: 8.0ms preprocess, 220.1ms inference, 9.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "27.473616536458334\n",
      "\n",
      "0: 480x640 1 person, 232.7ms\n",
      "Speed: 8.3ms preprocess, 232.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "24.416793619791665\n",
      "\n",
      "0: 480x640 1 person, 223.0ms\n",
      "Speed: 6.7ms preprocess, 223.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "23.226243489583332\n",
      "\n",
      "0: 480x640 1 person, 291.3ms\n",
      "Speed: 12.0ms preprocess, 291.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "29.404251302083335\n",
      "\n",
      "0: 480x640 1 person, 219.7ms\n",
      "Speed: 8.3ms preprocess, 219.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "18.967867838541668\n",
      "\n",
      "0: 480x640 1 person, 208.3ms\n",
      "Speed: 1.7ms preprocess, 208.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "20.537913411458334\n",
      "\n",
      "0: 480x640 1 person, 222.2ms\n",
      "Speed: 2.5ms preprocess, 222.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "25.314680989583334\n",
      "\n",
      "0: 480x640 1 person, 215.4ms\n",
      "Speed: 8.4ms preprocess, 215.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "26.175172526041667\n",
      "\n",
      "0: 480x640 1 person, 220.3ms\n",
      "Speed: 0.0ms preprocess, 220.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "28.7554296875\n",
      "\n",
      "0: 480x640 1 person, 213.1ms\n",
      "Speed: 4.0ms preprocess, 213.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "29.59880859375\n",
      "\n",
      "0: 480x640 1 person, 209.3ms\n",
      "Speed: 4.7ms preprocess, 209.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO as yolo\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "## Function to detect the difference between two frames.\n",
    "def diff(prev, frame):\n",
    "    prev = cv.cvtColor(prev, cv.COLOR_BGR2GRAY)\n",
    "    frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    # Compute the Mean Squared Error (MSE)\n",
    "    mse = ((prev - frame) ** 2).mean()\n",
    "    return mse\n",
    "\n",
    "## Function for angle calculation\n",
    "def angle_calc(p1: list, p2: list, p3: list):\n",
    "    if (p1.all() == 0) or (p2.all() == 0) or (p3.all() == 0):\n",
    "        return -1\n",
    "    v1 = p1 - p2\n",
    "    v2 = p3 - p2\n",
    "    cos_theta = (np.dot(v1, v2)) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n",
    "    # print(\"p1\",p1,\"p2\",p2,\"p3\",p3,\"v1\",v1,\"v2\",v2,\"Cos_theta\",cos_theta)\n",
    "    theta = np.arccos(cos_theta)  # angle in radians\n",
    "    angle = abs(theta * 180.0 / np.pi)  # angle in degree\n",
    "    if angle > 180:\n",
    "        angle = 360 - angle\n",
    "    return int(angle)\n",
    "\n",
    "## Calculating 8 important angles for pose detection.\n",
    "### This function is specific to tis code application.\n",
    "def joint_angles(joint: list):\n",
    "    joints = np.zeros((17, 2))\n",
    "    joints[: joint.shape[0], : joint.shape[1]] = joint\n",
    "    out = []\n",
    "    out.append(angle_calc(joints[9], joints[7], joints[5]))  ### Angle 1 on right side\n",
    "    out.append(angle_calc(joints[10], joints[8], joints[6]))  ### Angle 2 on left\n",
    "    out.append(angle_calc(joints[7], joints[5], joints[11]))  ### Angle 3 on right\n",
    "    out.append(angle_calc(joints[8], joints[6], joints[12]))  ### Angle 4 on left\n",
    "    out.append(angle_calc(joints[5], joints[11], joints[13]))  ### Angle 5 on right\n",
    "    out.append(angle_calc(joints[6], joints[12], joints[14]))  ### Angle 6 on left\n",
    "    out.append(angle_calc(joints[11], joints[13], joints[15]))  ### Angle 7 on right\n",
    "    out.append(angle_calc(joints[12], joints[14], joints[16]))  ### Angle 8 on left\n",
    "    return out\n",
    "\n",
    "## Special Cases for this code to determine the pose names.\n",
    "def pose(flexion_angles: list):\n",
    "    l = len(flexion_angles)\n",
    "    up = flexion_angles[: int(l / 2)]\n",
    "    down = flexion_angles[int(l / 2) :]\n",
    "    final = []\n",
    "    text = 0\n",
    "\n",
    "    def is_between(value, min_val, max_val):\n",
    "        return all(min_val[i] <= value[i] <= max_val[i] for i in range(len(value)))\n",
    "\n",
    "    match up:  ## Hands Up condition\n",
    "        case _ if is_between(up, [0, 0, 160, 160], [0, 0, 180, 180]):\n",
    "            text = \"Both Hands Up\"\n",
    "        case _ if is_between(up, [0, 0, 160, 0], [0, 0, 180, 0]):\n",
    "            text = \"Right Hand up\"\n",
    "        case _ if is_between(up, [0, 0, 0, 160], [0, 0, 0, 180]):\n",
    "            text = \"Left Hand Up\"\n",
    "\n",
    "    if text != 0:\n",
    "        final.append(text)\n",
    "        text = 0\n",
    "\n",
    "    match up:  ## Hands Raised condition\n",
    "        case _ if is_between(up, [80, 80, 80, 80], [100, 100, 100, 100]):\n",
    "            text = \"Both Hands Raised Up\"\n",
    "        case _ if is_between(up, [80, 0, 80, 0], [100, 0, 100, 0]):\n",
    "            text = \"Right Hand Raised up\"\n",
    "        case _ if is_between(up, [0, 80, 0, 80], [0, 100, 0, 100]):\n",
    "            text = \"Left Hand Raised Up\"\n",
    "\n",
    "    if text != 0:\n",
    "        final.append(text)\n",
    "        text = 0\n",
    "\n",
    "    match up:  ## Hands Horizontal condition\n",
    "        case _ if is_between(up, [160, 160, 80, 80], [180, 180, 100, 100]):\n",
    "            text = \"Both hands are horizontal\"\n",
    "        case _ if is_between(up, [160, 160, 0, 80], [180, 180, 20, 100]):\n",
    "            text = \"Right hand is horizontal\"\n",
    "        case _ if is_between(up, [160, 160, 80, 0], [180, 180, 100, 20]):\n",
    "            text = \"Left hand is horizontal\"\n",
    "\n",
    "    if text != 0:\n",
    "        final.append(text)\n",
    "        text = 0\n",
    "\n",
    "    match up:  ## Hands Down condition\n",
    "        case _ if is_between(up, [130, 130, 0, 0], [180, 180, 30, 30]):\n",
    "            text = \"Both Hands Down\"\n",
    "        case _ if is_between(up, [0, 130, 0, 0], [0, 180, 30, 30]):\n",
    "            text = \"Left Hand Down\"\n",
    "        case _ if is_between(up, [130, 0, 0, 0], [180, 30, 30, 30]):\n",
    "            text = \"Right Hand Down\"\n",
    "\n",
    "    if text != 0:\n",
    "        final.append(text)\n",
    "        text = 0\n",
    "\n",
    "    match down:  ## Legs position condition\n",
    "        case _ if is_between(down, [80, 80, -1, -1], [100, 100, 180, 180]):\n",
    "            text = \"Sitting Down\"\n",
    "        case _ if is_between(down, [160, 160, -1, -1], [180, 180, 180, 180]):\n",
    "            text = \"Standing pose\"\n",
    "        case _ if is_between(down, [80, 160, -1, 160], [100, 180, 180, 180]):\n",
    "            text = \"Standing on left leg\"\n",
    "        case _ if is_between(down, [160, 80, 160, -1], [180, 100, 180, 180]):\n",
    "            text = \"Standing on Right leg\"\n",
    "\n",
    "    if text != 0:\n",
    "        final.append(text)\n",
    "        text = 0\n",
    "    return final\n",
    "\n",
    "# List of common colors in OpenCV (BGR format)\n",
    "colors = [\n",
    "    (0, 0, 255),      # Red\n",
    "    (0, 255, 0),      # Green\n",
    "    # (255, 0, 0),      # Blue\n",
    "    (0, 255, 255),    # Yellow\n",
    "    (255, 255, 0),    # Cyan\n",
    "    (255, 0, 255),    # Magenta\n",
    "    (128, 128, 128),  # Gray\n",
    "    (50, 50, 50),     # Dark Gray\n",
    "    (200, 200, 200),  # Light Gray\n",
    "    (0, 0, 128),      # Maroon\n",
    "    (0, 128, 128),    # Olive\n",
    "    (128, 0, 128),    # Purple\n",
    "    (128, 128, 0),    # Teal\n",
    "    (128, 0, 0),      # Navy\n",
    "    (0, 165, 255),    # Orange\n",
    "    (19, 69, 139),    # Brown\n",
    "    (203, 192, 255),  # Pink\n",
    "    (230, 216, 173)   # Light Blue\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Main function starts here to determine the Pose of the subject.\n",
    "\"\"\"\n",
    "\n",
    "model = yolo(\"yolov8n-pose.pt\", task=\"pose\")  ## defining the model\n",
    "\n",
    "# cap = cv.VideoCapture(\"test_video3.mp4\")  ## to use a video from the device\n",
    "cap = cv.VideoCapture(0)  ## To capture from the webcam\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if cap.isOpened() == False:\n",
    "    print(\"Error opening video file\")\n",
    "\n",
    "ret, prev = cap.read()\n",
    "\n",
    "thresh = 5\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        if diff(prev, frame) > thresh:\n",
    "            print(diff(prev, frame))\n",
    "            results = model(source=frame)\n",
    "            frame1 = results[0].plot()\n",
    "            # thresh += 1\n",
    "        else:\n",
    "            frame1 = results[0].plot(\n",
    "                img=frame\n",
    "            )  ## to print the annotations on the original image without feeding it into the model.\n",
    "            print(diff(prev, frame))\n",
    "            # thresh -= 0.5\n",
    "        prev = frame\n",
    "        joints = np.array(results[0].keypoints.xy).astype(\n",
    "            int\n",
    "        )  # saving the coordinates in joints variable as int\n",
    "\n",
    "        img_text = np.zeros((frame1.shape[0], frame1.shape[1] + 200, frame1.shape[2]), dtype=np.uint8)\n",
    "        img_text[: frame1.shape[0], : frame1.shape[1], : frame1.shape[2]] = frame1\n",
    "        cv.putText(\n",
    "            img_text,\n",
    "            text=\"Pose:\",\n",
    "            org=(frame1.shape[1], 20),\n",
    "            fontFace=cv.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=0.5,\n",
    "            color=(255, 255, 255),\n",
    "            thickness=1,\n",
    "            lineType=cv.LINE_AA,\n",
    "        )\n",
    "        var = 1\n",
    "        for j in range(len(joints)): ### This loop ensures to detect pose for more than one persons.\n",
    "            out = joint_angles(joints[j])\n",
    "            final = pose(out) ### text variable containg the pose names.\n",
    "            cv.putText(\n",
    "            img_text,\n",
    "            text=\"Person \"+str(j+1)+\" :\",\n",
    "            org=(frame1.shape[1], (var+1)*20),\n",
    "            fontFace=cv.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=0.5,\n",
    "            color=colors[var],\n",
    "            thickness=1,\n",
    "            lineType=cv.LINE_AA,\n",
    "        )\n",
    "            var += 1\n",
    "            for i in range(len(final)):\n",
    "                cv.putText(\n",
    "                    img_text,\n",
    "                    text=final[i],\n",
    "                    org=(frame1.shape[1], 20 * (var + 1)),\n",
    "                    fontFace=cv.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=0.5,\n",
    "                    color=colors[i+j],\n",
    "                    thickness=1,\n",
    "                    lineType=cv.LINE_AA,\n",
    "                )\n",
    "                var += 1\n",
    "        cv.imshow(\"Frame\", img_text)\n",
    "        # Break the loop on 'q' key press\n",
    "        if cv.waitKey(20) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we use it for two cameras for person detection and identification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 315.9ms\n",
      "Speed: 0.0ms preprocess, 315.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 244.9ms\n",
      "Speed: 6.7ms preprocess, 244.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "15.860406901041667 107.36931315104167\n",
      "\n",
      "0: 480x640 1 person, 360.1ms\n",
      "Speed: 6.5ms preprocess, 360.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 246.6ms\n",
      "Speed: 0.0ms preprocess, 246.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "29.985231119791667 31.803304036458332\n",
      "\n",
      "0: 480x640 1 person, 317.6ms\n",
      "Speed: 0.0ms preprocess, 317.6ms inference, 8.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 231.8ms\n",
      "Speed: 5.6ms preprocess, 231.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "24.578938802083332 31.28986328125\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO as yolo\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Function to detect the difference between two frames.\n",
    "def diff(prev, frame):\n",
    "    prev = cv.cvtColor(prev, cv.COLOR_BGR2GRAY)\n",
    "    frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    mse = ((prev - frame) ** 2).mean()\n",
    "    return mse\n",
    "\n",
    "# Function for angle calculation\n",
    "def angle_calc(p1: list, p2: list, p3: list):\n",
    "    if (p1.all() == 0) or (p2.all() == 0) or (p3.all() == 0):\n",
    "        return -1\n",
    "    v1 = p1 - p2\n",
    "    v2 = p3 - p2\n",
    "    cos_theta = (np.dot(v1, v2)) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n",
    "    theta = np.arccos(cos_theta)\n",
    "    angle = abs(theta * 180.0 / np.pi)\n",
    "    if angle > 180:\n",
    "        angle = 360 - angle\n",
    "    return int(angle)\n",
    "\n",
    "# Calculating 8 important angles for pose detection.\n",
    "def joint_angles(joint: list):\n",
    "    joints = np.zeros((17, 2))\n",
    "    joints[: joint.shape[0], : joint.shape[1]] = joint\n",
    "    out = []\n",
    "    out.append(angle_calc(joints[9], joints[7], joints[5]))  # Angle 1 on right side\n",
    "    out.append(angle_calc(joints[10], joints[8], joints[6]))  # Angle 2 on left\n",
    "    out.append(angle_calc(joints[7], joints[5], joints[11]))  # Angle 3 on right\n",
    "    out.append(angle_calc(joints[8], joints[6], joints[12]))  # Angle 4 on left\n",
    "    out.append(angle_calc(joints[5], joints[11], joints[13]))  # Angle 5 on right\n",
    "    out.append(angle_calc(joints[6], joints[12], joints[14]))  # Angle 6 on left\n",
    "    out.append(angle_calc(joints[11], joints[13], joints[15]))  # Angle 7 on right\n",
    "    out.append(angle_calc(joints[12], joints[14], joints[16]))  # Angle 8 on left\n",
    "    return out\n",
    "\n",
    "# Special Cases to determine the pose names.\n",
    "def pose(flexion_angles: list):\n",
    "    l = len(flexion_angles)\n",
    "    up = flexion_angles[: int(l / 2)]\n",
    "    down = flexion_angles[int(l / 2) :]\n",
    "    final = []\n",
    "    text = 0\n",
    "\n",
    "    def is_between(value, min_val, max_val):\n",
    "        return all(min_val[i] <= value[i] <= max_val[i] for i in range(len(value)))\n",
    "\n",
    "    match up:\n",
    "        case _ if is_between(up, [0, 0, 160, 160], [0, 0, 180, 180]):\n",
    "            text = \"Both Hands Up\"\n",
    "        case _ if is_between(up, [0, 0, 160, 0], [0, 0, 180, 0]):\n",
    "            text = \"Right Hand up\"\n",
    "        case _ if is_between(up, [0, 0, 0, 160], [0, 0, 0, 180]):\n",
    "            text = \"Left Hand Up\"\n",
    "\n",
    "    if text != 0:\n",
    "        final.append(text)\n",
    "        text = 0\n",
    "\n",
    "    match up:\n",
    "        case _ if is_between(up, [80, 80, 80, 80], [100, 100, 100, 100]):\n",
    "            text = \"Both Hands Raised Up\"\n",
    "        case _ if is_between(up, [80, 0, 80, 0], [100, 0, 100, 0]):\n",
    "            text = \"Right Hand Raised up\"\n",
    "        case _ if is_between(up, [0, 80, 0, 80], [0, 100, 0, 100]):\n",
    "            text = \"Left Hand Raised Up\"\n",
    "\n",
    "    if text != 0:\n",
    "        final.append(text)\n",
    "        text = 0\n",
    "\n",
    "    match up:\n",
    "        case _ if is_between(up, [160, 160, 80, 80], [180, 180, 100, 100]):\n",
    "            text = \"Both hands are horizontal\"\n",
    "        case _ if is_between(up, [160, 160, 0, 80], [180, 180, 20, 100]):\n",
    "            text = \"Right hand is horizontal\"\n",
    "        case _ if is_between(up, [160, 160, 80, 0], [180, 180, 100, 20]):\n",
    "            text = \"Left hand is horizontal\"\n",
    "\n",
    "    if text != 0:\n",
    "        final.append(text)\n",
    "        text = 0\n",
    "\n",
    "    match up:\n",
    "        case _ if is_between(up, [130, 130, 0, 0], [180, 180, 30, 30]):\n",
    "            text = \"Both Hands Down\"\n",
    "        case _ if is_between(up, [0, 130, 0, 0], [0, 180, 30, 30]):\n",
    "            text = \"Left Hand Down\"\n",
    "        case _ if is_between(up, [130, 0, 0, 0], [180, 30, 30, 30]):\n",
    "            text = \"Right Hand Down\"\n",
    "\n",
    "    if text != 0:\n",
    "        final.append(text)\n",
    "        text = 0\n",
    "\n",
    "    match down:\n",
    "        case _ if is_between(down, [80, 80, -1, -1], [100, 100, 180, 180]):\n",
    "            text = \"Sitting Down\"\n",
    "        case _ if is_between(down, [160, 160, -1, -1], [180, 180, 180, 180]):\n",
    "            text = \"Standing pose\"\n",
    "        case _ if is_between(down, [80, 160, -1, 160], [100, 180, 180, 180]):\n",
    "            text = \"Standing on left leg\"\n",
    "        case _ if is_between(down, [160, 80, 160, -1], [180, 100, 180, 180]):\n",
    "            text = \"Standing on Right leg\"\n",
    "\n",
    "    if text != 0:\n",
    "        final.append(text)\n",
    "        text = 0\n",
    "    return final\n",
    "\n",
    "# List of common colors in OpenCV (BGR format)\n",
    "colors = [\n",
    "    (0, 0, 255),      # Red\n",
    "    (0, 255, 0),      # Green\n",
    "    (0, 255, 255),    # Yellow\n",
    "    (255, 255, 0),    # Cyan\n",
    "    (255, 0, 255),    # Magenta\n",
    "    (128, 128, 128),  # Gray\n",
    "    (50, 50, 50),     # Dark Gray\n",
    "    (200, 200, 200),  # Light Gray\n",
    "    (0, 0, 128),      # Maroon\n",
    "    (0, 128, 128),    # Olive\n",
    "    (128, 0, 128),    # Purple\n",
    "    (128, 128, 0),    # Teal\n",
    "    (128, 0, 0),      # Navy\n",
    "    (0, 165, 255),    # Orange\n",
    "    (19, 69, 139),    # Brown\n",
    "    (203, 192, 255),  # Pink\n",
    "    (230, 216, 173)   # Light Blue\n",
    "]\n",
    "\n",
    "# Main function starts here to determine the Pose of the subject.\n",
    "model = yolo(\"yolov8n-pose.pt\", task=\"pose\")\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if cap.isOpened() == False:\n",
    "    print(\"Error opening video file\")\n",
    "\n",
    "ret, prev = cap.read()\n",
    "\n",
    "thresh = 5\n",
    "frame_count = 0\n",
    "skip_frames = 2  # Process every 2nd frame\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame1 = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        frame_count += 1\n",
    "\n",
    "        # Skip frames for faster processing\n",
    "        if frame_count % skip_frames != 0:\n",
    "            continue\n",
    "\n",
    "        output_frame1 = None\n",
    "        output_frame2 = None\n",
    "\n",
    "        results1 = model(frame1)\n",
    "        output_frame1 = results1[0].plot(img=frame1)\n",
    "\n",
    "        ret, frame2 = cap.read()\n",
    "\n",
    "        if ret == True:\n",
    "            results2 = model(frame2)\n",
    "            output_frame2 = results2[0].plot(img=frame2)\n",
    "            print(diff(prev, frame1), diff(prev, frame2))\n",
    "            prev = frame2\n",
    "\n",
    "            joints1 = np.array(results1[0].keypoints.xy).astype(int)\n",
    "            joints2 = np.array(results2[0].keypoints.xy).astype(int)\n",
    "\n",
    "            # Resize frames to fit better on the screen\n",
    "            output_frame1 = cv.resize(output_frame1, (640, 420))\n",
    "            output_frame2 = cv.resize(output_frame2, (640, 420))\n",
    "\n",
    "            img_text1 = np.zeros((output_frame1.shape[0], output_frame1.shape[1] + 200, output_frame1.shape[2]), dtype=np.uint8)\n",
    "            img_text2 = np.zeros((output_frame2.shape[0], output_frame2.shape[1] + 200, output_frame2.shape[2]), dtype=np.uint8)\n",
    "\n",
    "            img_text1[: output_frame1.shape[0], : output_frame1.shape[1], : output_frame1.shape[2]] = output_frame1\n",
    "            img_text2[: output_frame2.shape[0], : output_frame2.shape[1], : output_frame2.shape[2]] = output_frame2\n",
    "\n",
    "            cv.putText(\n",
    "                img_text1,\n",
    "                text=\"Pose from Camera 1:\",\n",
    "                org=(output_frame1.shape[1], 20),\n",
    "                fontFace=cv.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=0.5,\n",
    "                color=(255, 255, 255),\n",
    "                thickness=1,\n",
    "                lineType=cv.LINE_AA,\n",
    "            )\n",
    "            cv.putText(\n",
    "                img_text2,\n",
    "                text=\"Pose from Camera 2:\",\n",
    "                org=(output_frame2.shape[1], 20),\n",
    "                fontFace=cv.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=0.5,\n",
    "                color=(255, 255, 255),\n",
    "                thickness=1,\n",
    "                lineType=cv.LINE_AA,\n",
    "            )\n",
    "\n",
    "            var = 1\n",
    "            for j in range(len(joints1)):  ### This loop ensures to detect pose for more than one person.\n",
    "                out = joint_angles(joints1[j])\n",
    "                final = pose(out)  ### text variable containing the pose names.\n",
    "                cv.putText(\n",
    "                    img_text1,\n",
    "                    text=\"Person \" + str(j + 1) + \" :\",\n",
    "                    org=(output_frame1.shape[1], (var + 1) * 20),\n",
    "                    fontFace=cv.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=0.5,\n",
    "                    color=colors[var],\n",
    "                    thickness=1,\n",
    "                    lineType=cv.LINE_AA,\n",
    "                )\n",
    "                var += 1\n",
    "                for i in range(len(final)):\n",
    "                    cv.putText(\n",
    "                        img_text1,\n",
    "                        text=final[i],\n",
    "                        org=(output_frame1.shape[1], 20 * (var + 1)),\n",
    "                        fontFace=cv.FONT_HERSHEY_SIMPLEX,\n",
    "                        fontScale=0.5,\n",
    "                        color=colors[i + j],\n",
    "                        thickness=1,\n",
    "                        lineType=cv.LINE_AA,\n",
    "                    )\n",
    "                    var += 1\n",
    "\n",
    "            var = 1\n",
    "            for j in range(len(joints2)):  ### This loop ensures to detect pose for more than one person.\n",
    "                out = joint_angles(joints2[j])\n",
    "                final = pose(out)  ### text variable containing the pose names.\n",
    "                cv.putText(\n",
    "                    img_text2,\n",
    "                    text=\"Person \" + str(j + 1) + \" :\",\n",
    "                    org=(output_frame2.shape[1], (var + 1) * 20),\n",
    "                    fontFace=cv.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=0.5,\n",
    "                    color=colors[var],\n",
    "                    thickness=1,\n",
    "                    lineType=cv.LINE_AA,\n",
    "                )\n",
    "                var += 1\n",
    "                for i in range(len(final)):\n",
    "                    cv.putText(\n",
    "                        img_text2,\n",
    "                        text=final[i],\n",
    "                        org=(output_frame2.shape[1], 20 * (var + 1)),\n",
    "                        fontFace=cv.FONT_HERSHEY_SIMPLEX,\n",
    "                        fontScale=0.5,\n",
    "                        color=colors[i + j],\n",
    "                        thickness=1,\n",
    "                        lineType=cv.LINE_AA,\n",
    "                    )\n",
    "                    var += 1\n",
    "\n",
    "            combined_frame = np.vstack((img_text1, img_text2))\n",
    "            cv.imshow(\"Frames\", combined_frame)\n",
    "            # Break the loop on 'q' key press\n",
    "            if cv.waitKey(20) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:**\n",
    "\n",
    "Though detection was possible identifying an individual uniquely was the issue. If person is recognized as person 1 he comes out of the frame and comes after another person the another person is identified as person 1 and he is identified as person 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suggestions:**\n",
    "\n",
    "Implement Object Tracking Algorithms:\n",
    "\n",
    "Use algorithms like Deep SORT or CSRT to maintain individual identities over time.\n",
    "Assign Unique Identifiers:\n",
    "\n",
    "Use Kalman Filters and feature matching (e.g., color histograms) to assign and maintain unique identifiers for each person.\n",
    "Combine Pose Information with Tracking:\n",
    "\n",
    "Integrate pose keypoints with tracking to better distinguish and track individuals.\n",
    "Improve Detection Consistency:\n",
    "\n",
    "Increase frame rate and apply temporal smoothing to stabilize tracking and reduce identity switching.\n",
    "Handle Occlusions:\n",
    "\n",
    "Implement occlusion handling techniques and consider using multiple cameras for different angles.\n",
    "Robust Data Association:\n",
    "\n",
    "Use the Hungarian algorithm and IoU calculations to match detections with tracked individuals accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
